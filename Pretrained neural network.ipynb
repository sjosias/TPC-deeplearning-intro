{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import PACSDataset\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a neural network\n",
    "We are able to load data from our custom datasets, now we need to train a neural network. We need the following:\n",
    "\n",
    "1. A neural network\n",
    "2. An optimiser\n",
    "3. A training loop that iterates through samples provided by a dataloader and uses the optimiser to update the neural networks weights\n",
    "\n",
    "I like to create a module for the neural network, and then a class that couples 2 and 3. Lets start the dataloader and then explore these concepts. \n",
    "\n",
    "## Train and Test dataloaders\n",
    "Normally, we'll have a training, validation and test set. The validation set is used for hyperparameter tuning. Since we won't do any hyperparameter tuning we can just go ahead and use the test set for evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files and transforms\n",
    "with open(\"train_files.txt\") as f:\n",
    "    file_names_train = f.read().splitlines()\n",
    "    \n",
    "with open(\"test_files.txt\") as f:\n",
    "    file_names_test = f.read().splitlines()\n",
    "\n",
    "# transforms\n",
    "# A great point to add data augmentations - you want to do class-preserving transformations\n",
    "# Rotations, Reflections etc\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize(size=(224, 224)),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "\n",
    "train_dataset = PACSDataset(file_names_train, transform)\n",
    "test_dataset = PACSDataset(file_names_test, transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network modules\n",
    "We will create a neural network modules. To illustrate the modularity of pytorch we will break it down into multiple modules.\n",
    "\n",
    "1. A featurizer: applies convolutions to extract \"useful\" features\n",
    "2. A classifier: fully connected net built on top of featurizer\n",
    "3. A network that combines the two: Allows us to easily swap out parts 1 or 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "\n",
    "\n",
    "# Classifier\n",
    "# We will hardcode the layers for now, but it is best to use parameters\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, num_classes: int = 7, num_features = 12544):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_features, 4096)\n",
    "        self.dropout = nn.Dropout2d(0.5)\n",
    "        self.fc2 = nn.Linear(4096, 1024)\n",
    "        self.fc3 = nn.Linear(1024, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Combination\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, num_classes: int = 7):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.net = models.resnet18(pretrained=True)\n",
    "        for param in self.net.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Parameters of newly constructed modules have requires_grad=True by default\n",
    "        num_ftrs = self.net.fc.in_features\n",
    "        self.net.fc = Classifier(num_classes=num_classes, num_features=num_ftrs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNClassifier(\n",
      "  (net): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Classifier(\n",
      "      (fc1): Linear(in_features=512, out_features=4096, bias=True)\n",
      "      (dropout): Dropout2d(p=0.5, inplace=False)\n",
      "      (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      (fc3): Linear(in_features=1024, out_features=7, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialise network\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNClassifier(num_classes=7)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training logic\n",
    "As mentioned before, we can fit the training logic into a class with a network as an attribute.\n",
    "\n",
    "We also need to consider hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "# batch_size = 64\n",
    "epochs = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 20 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shane Josias\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.979666  [    0/ 6993]\n",
      "loss: 1.937241  [ 1280/ 6993]\n",
      "loss: 1.936771  [ 2560/ 6993]\n",
      "loss: 1.928247  [ 3840/ 6993]\n",
      "loss: 1.870707  [ 5120/ 6993]\n",
      "loss: 1.898243  [ 6400/ 6993]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.917826  [    0/ 6993]\n",
      "loss: 1.888366  [ 1280/ 6993]\n",
      "loss: 1.891215  [ 2560/ 6993]\n",
      "loss: 1.868423  [ 3840/ 6993]\n",
      "loss: 1.852230  [ 5120/ 6993]\n",
      "loss: 1.862719  [ 6400/ 6993]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.865219  [    0/ 6993]\n",
      "loss: 1.880766  [ 1280/ 6993]\n",
      "loss: 1.835865  [ 2560/ 6993]\n",
      "loss: 1.841527  [ 3840/ 6993]\n",
      "loss: 1.838375  [ 5120/ 6993]\n",
      "loss: 1.837552  [ 6400/ 6993]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.807266  [    0/ 6993]\n",
      "loss: 1.795572  [ 1280/ 6993]\n",
      "loss: 1.793792  [ 2560/ 6993]\n",
      "loss: 1.773298  [ 3840/ 6993]\n",
      "loss: 1.754071  [ 5120/ 6993]\n",
      "loss: 1.772453  [ 6400/ 6993]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.779099  [    0/ 6993]\n",
      "loss: 1.760010  [ 1280/ 6993]\n",
      "loss: 1.759344  [ 2560/ 6993]\n",
      "loss: 1.768843  [ 3840/ 6993]\n",
      "loss: 1.781506  [ 5120/ 6993]\n",
      "loss: 1.761443  [ 6400/ 6993]\n",
      "Test Error: \n",
      " Accuracy: 38.6%, Avg loss: 1.734644 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 37.6%, Avg loss: 1.738841 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.763485  [    0/ 6993]\n",
      "loss: 1.717782  [ 1280/ 6993]\n",
      "loss: 1.714665  [ 2560/ 6993]\n",
      "loss: 1.662233  [ 3840/ 6993]\n",
      "loss: 1.751700  [ 5120/ 6993]\n",
      "loss: 1.668711  [ 6400/ 6993]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.683875  [    0/ 6993]\n",
      "loss: 1.613128  [ 1280/ 6993]\n",
      "loss: 1.707678  [ 2560/ 6993]\n",
      "loss: 1.705902  [ 3840/ 6993]\n",
      "loss: 1.627017  [ 5120/ 6993]\n",
      "loss: 1.683354  [ 6400/ 6993]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.579551  [    0/ 6993]\n",
      "loss: 1.633443  [ 1280/ 6993]\n",
      "loss: 1.616672  [ 2560/ 6993]\n",
      "loss: 1.576957  [ 3840/ 6993]\n",
      "loss: 1.577341  [ 5120/ 6993]\n",
      "loss: 1.527068  [ 6400/ 6993]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.641162  [    0/ 6993]\n",
      "loss: 1.590857  [ 1280/ 6993]\n",
      "loss: 1.533755  [ 2560/ 6993]\n",
      "loss: 1.571946  [ 3840/ 6993]\n",
      "loss: 1.569706  [ 5120/ 6993]\n",
      "loss: 1.600178  [ 6400/ 6993]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.565096  [    0/ 6993]\n",
      "loss: 1.626786  [ 1280/ 6993]\n",
      "loss: 1.514487  [ 2560/ 6993]\n",
      "loss: 1.490898  [ 3840/ 6993]\n",
      "loss: 1.462237  [ 5120/ 6993]\n",
      "loss: 1.548003  [ 6400/ 6993]\n",
      "Test Error: \n",
      " Accuracy: 55.6%, Avg loss: 1.494680 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.8%, Avg loss: 1.489908 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.501996  [    0/ 6993]\n",
      "loss: 1.448224  [ 1280/ 6993]\n",
      "loss: 1.476403  [ 2560/ 6993]\n",
      "loss: 1.480329  [ 3840/ 6993]\n",
      "loss: 1.520864  [ 5120/ 6993]\n",
      "loss: 1.426072  [ 6400/ 6993]\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.472125  [    0/ 6993]\n",
      "loss: 1.377164  [ 1280/ 6993]\n",
      "loss: 1.440573  [ 2560/ 6993]\n",
      "loss: 1.592138  [ 3840/ 6993]\n",
      "loss: 1.401709  [ 5120/ 6993]\n",
      "loss: 1.440649  [ 6400/ 6993]\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.441360  [    0/ 6993]\n",
      "loss: 1.458066  [ 1280/ 6993]\n",
      "loss: 1.397761  [ 2560/ 6993]\n",
      "loss: 1.442656  [ 3840/ 6993]\n",
      "loss: 1.361610  [ 5120/ 6993]\n",
      "loss: 1.321362  [ 6400/ 6993]\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.327863  [    0/ 6993]\n",
      "loss: 1.316530  [ 1280/ 6993]\n",
      "loss: 1.405970  [ 2560/ 6993]\n",
      "loss: 1.416102  [ 3840/ 6993]\n",
      "loss: 1.426378  [ 5120/ 6993]\n",
      "loss: 1.300521  [ 6400/ 6993]\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 1.354723  [    0/ 6993]\n",
      "loss: 1.355871  [ 1280/ 6993]\n",
      "loss: 1.380095  [ 2560/ 6993]\n",
      "loss: 1.382954  [ 3840/ 6993]\n",
      "loss: 1.268604  [ 5120/ 6993]\n",
      "loss: 1.263075  [ 6400/ 6993]\n",
      "Test Error: \n",
      " Accuracy: 61.4%, Avg loss: 1.266226 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Avg loss: 1.273511 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.208563  [    0/ 6993]\n",
      "loss: 1.141701  [ 1280/ 6993]\n",
      "loss: 1.391740  [ 2560/ 6993]\n",
      "loss: 1.321995  [ 3840/ 6993]\n",
      "loss: 1.290622  [ 5120/ 6993]\n",
      "loss: 1.263362  [ 6400/ 6993]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    if (t+1) % 5 == 0:\n",
    "        test_loop(train_dataloader, model, loss_fn)\n",
    "        test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
