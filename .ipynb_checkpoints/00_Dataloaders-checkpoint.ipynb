{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities and helper functions (optional)\n",
    "Adapted from [here](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#preparing-your-data-for-training-with-dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset(dataset, label_mapping):\n",
    "    figure = plt.figure(figsize=(10, 10))\n",
    "    cols, rows = 3, 3\n",
    "    for i in range(1, cols * rows + 1):\n",
    "        sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "        img, label = training_data[sample_idx]\n",
    "        img = img.numpy()\n",
    "        img = np.transpose(img, (1, 2, 0))\n",
    "        figure.add_subplot(rows, cols, i)\n",
    "        plt.title(label_mapping[label])\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(img.squeeze())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch DataSets and Dataloaders\n",
    "To facilitate model training, we need to be able to load data during the training step. This data often undergoes transformations to prepare it for training and we want to separate this loading and transformation logic from model training code. We use the following primities.\n",
    "\n",
    " 1. [torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset): Stores samples and their labels if supervised learning.\n",
    " 2. [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader): Wraps an iterable around the Dataset so that we can loop through samples. \n",
    "\n",
    "For more technical info on DataSet have a look [here](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset).\n",
    "\n",
    "Many tutorials use pre-existing [datasets](https://pytorch.org/vision/stable/datasets.html). We will build our own on a new dataset.\n",
    "\n",
    "## PACS Dataset\n",
    "\n",
    "We will use the dataset introduced in [Deeper, Broader and Artier Domain Generalisation](https://openaccess.thecvf.com/content_ICCV_2017/papers/Li_Deeper_Broader_and_ICCV_2017_paper.pdf).\n",
    "\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg .tg-baqh{text-align:center;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-baqh\">Dog</th>\n",
    "    <th class=\"tg-baqh\">Elephant</th>\n",
    "    <th class=\"tg-baqh\">Giraffe</th>\n",
    "    <th class=\"tg-baqh\">Guitar</th>\n",
    "    <th class=\"tg-baqh\">Horse</th>\n",
    "    <th class=\"tg-baqh\">House</th>\n",
    "    <th class=\"tg-baqh\">Person</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td class=\"tg-baqh\"><img src=\"images/dog.jpg\"></td>\n",
    "    <td class=\"tg-baqh\"><img src=\"images/elephant.jpg\"></td>\n",
    "    <td class=\"tg-baqh\"><img src=\"images/giraffe.jpg\"></td>\n",
    "    <td class=\"tg-baqh\"><img src=\"images/guitar.png\"></td>\n",
    "    <td class=\"tg-baqh\"><img src=\"images/horse.jpg\"></td>\n",
    "    <td class=\"tg-baqh\"><img src=\"images/house.jpg\"></td>\n",
    "    <td class=\"tg-baqh\"><img src=\"images/person.jpg\"></td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "We will get the labels from the file path: `Data/PACS/sketch/horse/n02374451_597-3.png`\n",
    "\n",
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class PACSDataset(Dataset):\n",
    "    # Required\n",
    "    def __init__(self, file_names, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            file_names (list): list of file names for images of the dataset (train or test split).\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.label_mapping = {\"dog\": 0,\n",
    "                              \"elephant\": 1, \n",
    "                              \"giraffe\": 2, \n",
    "                              \"guitar\": 3, \n",
    "                              \"horse\": 4, \n",
    "                              \"house\": 5, \n",
    "                              \"person\": 6\n",
    "                             }\n",
    "    \n",
    "        self.file_names = file_names\n",
    "        self.transform = transform\n",
    "\n",
    "    # Required   \n",
    "    def __len__(self):\n",
    "        pass\n",
    "    \n",
    "    # used to get label from file name\n",
    "    def get_label(self, file_name):\n",
    "       \n",
    "        pass\n",
    "    \n",
    "    # Required\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        # get image name using idx\n",
    "        \n",
    "        # read image\n",
    "        \n",
    "        # convert to tensor\n",
    "        \n",
    "        # get label\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "                    \n",
    "        return (image, label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some tips\n",
    "1. Make sure images are float32\n",
    "2. Make sure images are between 0 and 4\n",
    "\n",
    "### Does it work?\n",
    "It is always a good idea to test out a unit of code before integrating it into a larger piece of software. In our case we want to test the dataset before using it to train models. It is possible that low model performance can be due to bad input (corrupted images, incorrect encodings used, or blank images due to error in transformations, incorrect labels).\n",
    "\n",
    "One way to informally test a Dataset  is to visualise random samples. We will use the utility function defined at the top of the notebook for this purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read files from train_files\n",
    "\n",
    "# instantiate PACSDAtaset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label mapping used for plotting\n",
    "label_mapping = {0: \"dog\", 1: \"elephant\", 2: \"giraffe\", 3:\"guitar\", 4: \"horse\", 5:\"house\", 6: \"person\"}\n",
    "plot_dataset(training_data, label_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "When iterating through a DataSet, we have to do it one sample at a time. However, we want to train with something like mini-batch gradient descent so we need access to mini-batches during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(training_data, batch_size=9, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show sizes for a single batch\n",
    "image_batch, label_batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataloader(image_batch, label_batch, label_mapping):\n",
    "    figure = plt.figure(figsize=(10, 10))\n",
    "    cols, rows = 3, 3\n",
    "    for i, image in enumerate(image_batch):\n",
    "        img, label = image, label_batch[i]\n",
    "        img = img.numpy()\n",
    "        img = np.transpose(img, (1, 2, 0))\n",
    "        figure.add_subplot(rows, cols, i+1)\n",
    "        plt.title(label_mapping[int(label)])\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(img.squeeze())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataloader(image_batch, label_batch, label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
