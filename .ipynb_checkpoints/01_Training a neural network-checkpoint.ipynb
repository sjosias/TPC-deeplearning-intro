{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import PACSDataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a neural network\n",
    "We are able to load data from our custom datasets, now we need to train a neural network. We need the following:\n",
    "\n",
    "1. A neural network\n",
    "2. An optimiser\n",
    "3. A training loop that iterates through samples provided by a dataloader and uses the optimiser to update the neural networks weights\n",
    "\n",
    "\n",
    "## Train and Test dataloaders\n",
    "Normally, we'll have a training, validation and test set. The validation set is used for hyperparameter tuning. Since we won't do any hyperparameter tuning we can just go ahead and use the test set for evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files and transforms\n",
    "with open(\"train_files.txt\") as f:\n",
    "    file_names_train = f.read().splitlines()\n",
    "    \n",
    "with open(\"test_files.txt\") as f:\n",
    "    file_names_test = f.read().splitlines()\n",
    "\n",
    "# transforms\n",
    "# A great point to add data augmentations - you want to do class-preserving transformations\n",
    "# Rotations, Reflections etc\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize(size=(224, 224)),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "# Instantiate datasets\n",
    "\n",
    "# Instantiate dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network modules\n",
    "We will create a neural network modules. To illustrate the modularity of pytorch we will break it down into multiple modules.\n",
    "\n",
    "1. A featurizer: applies convolutions to extract \"useful\" features\n",
    "2. A classifier: fully connected net built on top of featurizer\n",
    "3. A network that combines the two: Allows us to easily swap out parts 1 or 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "\n",
    "\n",
    "# Featurizer\n",
    "class Featurizer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Featurizer, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.dropout = nn.Dropout2d(0.5)\n",
    "        self.conv2 = nn.Conv2d(16, 4, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "# Classifier\n",
    "# We will hardcode the layers for now, but it is best to use parameters\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, num_classes: int = 7):\n",
    "        super(Classifier, self).__init__()\n",
    "        # make a point here about the 12544\n",
    "        self.fc1 = nn.Linear(12544, 4096)\n",
    "        self.dropout = nn.Dropout2d(0.5)\n",
    "\n",
    "        self.fc2 = nn.Linear(4096, 1024)\n",
    "        self.fc3 = nn.Linear(1024, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Combination\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, num_classes: int = 7):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "      \n",
    "\n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise network and move to GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training logic\n",
    "As mentioned before, we can fit the training logic into a class with a network as an attribute.\n",
    "\n",
    "We also need to consider hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "epochs = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Move to device\n",
    "        \n",
    "        \n",
    "        # compute predictions\n",
    "        \n",
    "        # evaluate loss\n",
    "        \n",
    "\n",
    "        # Zero gradients\n",
    "        \n",
    "        \n",
    "        # Back propagation\n",
    "    \n",
    "        \n",
    "        # Gradient descent\n",
    "\n",
    "        # training progress\n",
    "        if batch % 20 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # turn of gradients - we don't need the extra memory footprint\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "# create loss_fn\n",
    "\n",
    "# create optimiser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# loop over epochs\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    if (t+1) % 5 == 0:\n",
    "        test_loop(train_dataloader, model, loss_fn)\n",
    "        test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
