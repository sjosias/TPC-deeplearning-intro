{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import PACSDataset\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a pretrained network\n",
    "We have created and trained a network from scratch. Now we will look at using a pretrained network. Pretrained networks are trained on image net and typically have good features. There are mainly two ways of doing so:\n",
    "\n",
    "1. Fixed feature extractor: We use a pretrained network, freeze weights and chop off the output layer. A new layer is appended at the end with randomly intialised weights. We train only the new layer.\n",
    "2. Fine-tuning: Instead of freezing the pretrained net, we also update its weights. The process is as follows.\n",
    "    * Start with a fixed feature extractor as above. \n",
    "    * Once the new layer is trained, unfreeze the entire net and train with a smaller learning rate. \n",
    "    \n",
    "\n",
    "## Train and Test dataloaders\n",
    "Normally, we'll have a training, validation and test set. The validation set is used for hyperparameter tuning. Since we won't do any hyperparameter tuning we can just go ahead and use the test set for evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files and transforms\n",
    "with open(\"train_files.txt\") as f:\n",
    "    file_names_train = f.read().splitlines()\n",
    "    \n",
    "with open(\"test_files.txt\") as f:\n",
    "    file_names_test = f.read().splitlines()\n",
    "\n",
    "# transforms\n",
    "# A great point to add data augmentations - you want to do class-preserving transformations\n",
    "# Rotations, Reflections etc\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize(size=(224, 224)),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "\n",
    "train_dataset = PACSDataset(file_names_train, transform)\n",
    "test_dataset = PACSDataset(file_names_test, transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network modules\n",
    "We will create neural network modules.\n",
    "\n",
    "1. A classifier: fully connected net built on top of featurizer\n",
    "2. A CNNClassifier which uses a pretrained net with the above classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "\n",
    "\n",
    "# Classifier\n",
    "# We will hardcode the layers for now, but it is best to use parameters\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, num_classes: int = 7, num_features = 12544):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_features, 4096)\n",
    "        self.dropout = nn.Dropout2d(0.5)\n",
    "        self.fc2 = nn.Linear(4096, 1024)\n",
    "        self.fc3 = nn.Linear(1024, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# \n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, num_classes: int = 7):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        # introduce resnet and freeze parameters\n",
    "        \n",
    "        # Add newly classifier\n",
    "        # Parameters of newly constructed modules have requires_grad=True by default\n",
    "        num_ftrs = self.net.fc.in_features\n",
    "        self.net.fc = ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialise network\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNClassifier(num_classes=7)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training logic\n",
    "As mentioned before, we can fit the training logic into a class with a network as an attribute.\n",
    "\n",
    "We also need to consider hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "# batch_size = 64\n",
    "epochs = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 20 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    if (t+1) % 5 == 0:\n",
    "        test_loop(train_dataloader, model, loss_fn)\n",
    "        test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
