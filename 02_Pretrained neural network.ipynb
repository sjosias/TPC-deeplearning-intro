{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import PACSDataset\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a pretrained network\n",
    "We have created and trained a network from scratch. Now we will look at using a pretrained network. Pretrained networks are trained on image net and typically have good features. There are mainly two ways of doing so:\n",
    "\n",
    "1. Fixed feature extractor: We use a pretrained network, freeze weights and chop off the output layer. A new layer is appended at the end with randomly intialised weights. We train only the new layer.\n",
    "2. Fine-tuning: Instead of freezing the pretrained net, we also update its weights. The process is as follows.\n",
    "    * Start with a fixed feature extractor as above. \n",
    "    * Once the new layer is trained, unfreeze the entire net and train with a smaller learning rate. \n",
    "\n",
    "Read more at Francois Chollet's  [Keras transfer learning tutorial](https://keras.io/guides/transfer_learning/#finetuning). He does a great job at explaining the different ways we can do transfer learning and the motivations behind them.\n",
    "\n",
    "Another great resource is Stanfords [CS231n transfer learning](https://cs231n.github.io/transfer-learning/) page.\n",
    "\n",
    "## Train and Test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files and transforms\n",
    "with open(\"train_files.txt\") as f:\n",
    "    file_names_train = f.read().splitlines()\n",
    "    \n",
    "with open(\"test_files.txt\") as f:\n",
    "    file_names_test = f.read().splitlines()\n",
    "\n",
    "# transforms\n",
    "# A great point to add data augmentations - you want to do class-preserving transformations\n",
    "# Rotations, Reflections etc\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize(size=(224, 224)),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "\n",
    "train_dataset = PACSDataset(file_names_train, transform)\n",
    "test_dataset = PACSDataset(file_names_test, transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network modules\n",
    "We will create neural network modules.\n",
    "\n",
    "1. A classifier: fully connected net built on top of featurizer\n",
    "2. A CNNClassifier which uses a pretrained net with the above classifier. There are many pretrained networks available to choose from with different tradeoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "\n",
    "\n",
    "# Classifier\n",
    "# We will hardcode the layers for now, but it is best to use parameters\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, num_classes: int = 7, num_features = 12544):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_features, 4096)\n",
    "        self.dropout = nn.Dropout2d(0.5)\n",
    "        self.fc2 = nn.Linear(4096, 1024)\n",
    "        self.fc3 = nn.Linear(1024, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# \n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, num_classes: int = 7):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        # introduce resnet and freeze parameters\n",
    "        self.net = models.resnet18(pretrained = True)\n",
    "        for param in self.net.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Add newly classifier\n",
    "        num_ftrs = self.net.fc.in_features\n",
    "        # Parameters of newly constructed modules have requires_grad=True by default\n",
    "        self.net.fc = Classifier(num_classes=num_classes, num_features=num_ftrs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNClassifier(\n",
      "  (net): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Classifier(\n",
      "      (fc1): Linear(in_features=512, out_features=4096, bias=True)\n",
      "      (dropout): Dropout2d(p=0.5, inplace=False)\n",
      "      (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      (fc3): Linear(in_features=1024, out_features=7, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialise network\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNClassifier(num_classes=7)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training logic\n",
    "As mentioned before, we can fit the training logic into a class with a network as an attribute.\n",
    "\n",
    "We also need to consider hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "# batch_size = 64\n",
    "epochs = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 20 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shane Josias\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.949770  [    0/ 6993]\n",
      "loss: 1.952374  [ 1280/ 6993]\n",
      "loss: 1.928283  [ 2560/ 6993]\n",
      "loss: 1.918936  [ 3840/ 6993]\n",
      "loss: 1.910475  [ 5120/ 6993]\n",
      "loss: 1.900642  [ 6400/ 6993]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.927967  [    0/ 6993]\n",
      "loss: 1.867704  [ 1280/ 6993]\n",
      "loss: 1.961480  [ 2560/ 6993]\n",
      "loss: 1.876495  [ 3840/ 6993]\n",
      "loss: 1.838209  [ 5120/ 6993]\n",
      "loss: 1.856876  [ 6400/ 6993]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.862854  [    0/ 6993]\n",
      "loss: 1.874785  [ 1280/ 6993]\n",
      "loss: 1.843776  [ 2560/ 6993]\n",
      "loss: 1.856889  [ 3840/ 6993]\n",
      "loss: 1.835270  [ 5120/ 6993]\n",
      "loss: 1.852608  [ 6400/ 6993]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.846892  [    0/ 6993]\n",
      "loss: 1.828829  [ 1280/ 6993]\n",
      "loss: 1.776567  [ 2560/ 6993]\n",
      "loss: 1.798976  [ 3840/ 6993]\n",
      "loss: 1.838850  [ 5120/ 6993]\n",
      "loss: 1.769426  [ 6400/ 6993]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.725686  [    0/ 6993]\n",
      "loss: 1.784106  [ 1280/ 6993]\n",
      "loss: 1.752797  [ 2560/ 6993]\n",
      "loss: 1.841489  [ 3840/ 6993]\n",
      "loss: 1.744029  [ 5120/ 6993]\n",
      "loss: 1.723945  [ 6400/ 6993]\n",
      "Test Error: \n",
      " Accuracy: 38.9%, Avg loss: 1.744430 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.749290 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.725312  [    0/ 6993]\n",
      "loss: 1.751009  [ 1280/ 6993]\n",
      "loss: 1.708020  [ 2560/ 6993]\n",
      "loss: 1.727554  [ 3840/ 6993]\n",
      "loss: 1.706756  [ 5120/ 6993]\n",
      "loss: 1.744955  [ 6400/ 6993]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.724648  [    0/ 6993]\n",
      "loss: 1.675422  [ 1280/ 6993]\n",
      "loss: 1.676036  [ 2560/ 6993]\n",
      "loss: 1.673358  [ 3840/ 6993]\n",
      "loss: 1.662025  [ 5120/ 6993]\n",
      "loss: 1.659443  [ 6400/ 6993]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.674749  [    0/ 6993]\n",
      "loss: 1.628707  [ 1280/ 6993]\n",
      "loss: 1.623458  [ 2560/ 6993]\n",
      "loss: 1.611951  [ 3840/ 6993]\n",
      "loss: 1.609281  [ 5120/ 6993]\n",
      "loss: 1.622423  [ 6400/ 6993]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.579331  [    0/ 6993]\n",
      "loss: 1.582866  [ 1280/ 6993]\n",
      "loss: 1.605745  [ 2560/ 6993]\n",
      "loss: 1.571095  [ 3840/ 6993]\n",
      "loss: 1.604969  [ 5120/ 6993]\n",
      "loss: 1.589469  [ 6400/ 6993]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.497620  [    0/ 6993]\n",
      "loss: 1.551304  [ 1280/ 6993]\n",
      "loss: 1.566316  [ 2560/ 6993]\n",
      "loss: 1.554376  [ 3840/ 6993]\n",
      "loss: 1.553718  [ 5120/ 6993]\n",
      "loss: 1.493525  [ 6400/ 6993]\n",
      "Test Error: \n",
      " Accuracy: 54.6%, Avg loss: 1.507838 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 54.7%, Avg loss: 1.512614 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.508448  [    0/ 6993]\n",
      "loss: 1.473161  [ 1280/ 6993]\n",
      "loss: 1.424874  [ 2560/ 6993]\n",
      "loss: 1.517186  [ 3840/ 6993]\n",
      "loss: 1.474187  [ 5120/ 6993]\n",
      "loss: 1.365113  [ 6400/ 6993]\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.482247  [    0/ 6993]\n",
      "loss: 1.488776  [ 1280/ 6993]\n",
      "loss: 1.439068  [ 2560/ 6993]\n",
      "loss: 1.420580  [ 3840/ 6993]\n",
      "loss: 1.445881  [ 5120/ 6993]\n",
      "loss: 1.333717  [ 6400/ 6993]\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.350866  [    0/ 6993]\n",
      "loss: 1.307736  [ 1280/ 6993]\n",
      "loss: 1.453162  [ 2560/ 6993]\n",
      "loss: 1.351799  [ 3840/ 6993]\n",
      "loss: 1.319970  [ 5120/ 6993]\n",
      "loss: 1.316884  [ 6400/ 6993]\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.401455  [    0/ 6993]\n",
      "loss: 1.459726  [ 1280/ 6993]\n",
      "loss: 1.299934  [ 2560/ 6993]\n",
      "loss: 1.275749  [ 3840/ 6993]\n",
      "loss: 1.372378  [ 5120/ 6993]\n",
      "loss: 1.325494  [ 6400/ 6993]\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 1.359978  [    0/ 6993]\n",
      "loss: 1.374279  [ 1280/ 6993]\n",
      "loss: 1.236004  [ 2560/ 6993]\n",
      "loss: 1.310501  [ 3840/ 6993]\n",
      "loss: 1.487217  [ 5120/ 6993]\n",
      "loss: 1.277586  [ 6400/ 6993]\n",
      "Test Error: \n",
      " Accuracy: 60.0%, Avg loss: 1.288030 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 1.291231 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.353941  [    0/ 6993]\n",
      "loss: 1.217861  [ 1280/ 6993]\n",
      "loss: 1.301327  [ 2560/ 6993]\n",
      "loss: 1.293208  [ 3840/ 6993]\n",
      "loss: 1.177205  [ 5120/ 6993]\n",
      "loss: 1.333848  [ 6400/ 6993]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    if (t+1) % 5 == 0:\n",
    "        test_loop(train_dataloader, model, loss_fn)\n",
    "        test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
